{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tiny llamas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(512, 64)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-4): 5 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wk): Linear(in_features=64, out_features=32, bias=False)\n",
       "        (wv): Linear(in_features=64, out_features=32, bias=False)\n",
       "        (wo): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=64, out_features=172, bias=False)\n",
       "        (w2): Linear(in_features=172, out_features=64, bias=False)\n",
       "        (w3): Linear(in_features=64, out_features=172, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=64, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import *\n",
    "device = \"cpu\"\n",
    "ckpt_name = \"stories260K/stories260K\"\n",
    "ckpt_path = \"tinyllamas/\"+ckpt_name+\".pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "\n",
    "model_args = dict()\n",
    "\n",
    "for k in [\"dim\", \"n_layers\", \"n_heads\", \"n_kv_heads\", \"vocab_size\", \"multiple_of\", \"max_seq_len\"]:\n",
    "    model_args[k] = checkpoint_model_args[k]\n",
    "# create the model\n",
    "gptconf = ModelArgs(**model_args)\n",
    "model = Transformer(gptconf)\n",
    "state_dict = checkpoint[\"model\"]\n",
    "# fix the keys of the state dictionary :(\n",
    "# honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "iter_num = checkpoint[\"iter_num\"]\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1, 317, 410, 293, 393]])\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "vocab_size = gptconf.vocab_size\n",
    "# tokenizer_model = \"tokenizer.model\"\n",
    "tokenizer_model = \"tinyllamas/stories260K/tok512.model\"\n",
    "enc = Tokenizer(tokenizer_model=tokenizer_model)\n",
    "\n",
    "start = \"Lily is happy\"\n",
    "\n",
    "start_ids = enc.encode(start, bos=True, eos=False)\n",
    "input_data = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lily is happy to see her mom. She liked to play with her toys and see what was inside. She saw a big box with a big box. She wanted to play with it.\n",
      "\"Look, Mommy, can I play with you?\" Lily asked.\n",
      "\"Yes, I can help you.\"\n",
      "\"OK,\" Lily said. \"It's a small box. It is a small box. It is a small box.\"\n",
      "Lily and Mommy went to the box. They saw a big box. They wanted to play with the box. They did not know what to do.\n",
      "\"Wow, it's so pretty!\" Lily said.\n",
      "\"Yes, it's a good boy. You are a good friend. You are a good friend. You are a good friend.\"\n",
      "Lily\n",
      "---------------\n",
      "Lily is happy to see her mom. She liked to play with her toys and see what was inside. She saw a big box with a big box. She wanted to play with it.\n",
      "\"Look, Mommy, can I play with you?\" Lily asked.\n",
      "\"Yes, I can help you.\"\n",
      "\"OK,\" Lily said. \"It's a small box. It is a small box. It is a small box.\"\n",
      "Lily and Mommy went to the box. They saw a big box. They wanted to play with the box. They did not know what to do.\n",
      "\"Wow, it's so pretty!\" Lily said.\n",
      "\"Yes, it's a good boy. You are a good friend. You are a good friend. You are a good friend.\"\n",
      "Lily\n",
      "---------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(512, 64)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-4): 5 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "        (wk): DynamicQuantizedLinear(in_features=64, out_features=32, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "        (wv): DynamicQuantizedLinear(in_features=64, out_features=32, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "        (wo): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): DynamicQuantizedLinear(in_features=64, out_features=172, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "        (w2): DynamicQuantizedLinear(in_features=172, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "        (w3): DynamicQuantizedLinear(in_features=64, out_features=172, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): DynamicQuantizedLinear(in_features=64, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8 = torch.ao.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "checkpoint_int8 = {\n",
    "    \"model\": model_int8.state_dict(),\n",
    "    \"model_args\": model_args,\n",
    "    \"iter_num\": iter_num,\n",
    "    \"best_val_loss\": best_val_loss,\n",
    "}\n",
    "out_dir = \"quant/\"\n",
    "torch.save(checkpoint, os.path.join(out_dir, ckpt_name+\"_quant.pt\"))\n",
    "from contextlib import nullcontext\n",
    "\n",
    "num_samples = 1\n",
    "max_new_tokens = 256\n",
    "top_k = 300\n",
    "\n",
    "with torch.no_grad():\n",
    "    with nullcontext():\n",
    "            for k in range(num_samples):\n",
    "                y = model.generate(input_data, max_new_tokens, temperature=0.0, top_k=top_k)\n",
    "                print(enc.decode(y[0].tolist()))\n",
    "                print('---------------')\n",
    "\n",
    "                y_q = model_int8.generate(input_data, max_new_tokens, temperature=0.0, top_k=top_k)\n",
    "                print(enc.decode(y[0].tolist()))\n",
    "                print('---------------')\n",
    "model_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama2.c internal quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from export import model_export\n",
    "\n",
    "model_export(model, os.path.join(out_dir, ckpt_name+\"_quant.bin\"), version=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
